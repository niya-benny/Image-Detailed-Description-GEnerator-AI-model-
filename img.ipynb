{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentencepiece bitsandbytes accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ce60cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47490b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbbddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gtts --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e47011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optimum auto-gptq --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f0263",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optimum --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa42c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4066aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5m.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ecb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate --quiet\n",
    "!pip install rouge_score --quiet\n",
    "!pip install textstat --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "from gtts import gTTS\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Blip2Processor, Blip2ForConditionalGeneration\n",
    ")\n",
    "from ultralytics import YOLO\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a8c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa6021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = YOLO(\"yolov5m.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float32\n",
    ").to(device)\n",
    "blip_model.gradient_checkpointing_enable()\n",
    "blip_model.language_model = prepare_model_for_kbit_training(blip_model.language_model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "blip_model.language_model = get_peft_model(blip_model.language_model, lora_config)\n",
    "blip_model.language_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdda868",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model_name = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_name, use_fast=True)\n",
    "llama_model = AutoGPTQForCausalLM.from_quantized(\n",
    "    llama_model_name,\n",
    "    device=\"cuda:0\",\n",
    "    torch_dtype=torch.float32,\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True,\n",
    "    device_map=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89440355",
   "metadata": {},
   "outputs": [],
   "source": [
    "nllb_model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "nllb_tokenizer = AutoTokenizer.from_pretrained(nllb_model_name)\n",
    "nllb_model = AutoModelForSeq2SeqLM.from_pretrained(nllb_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b89893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_blip2(blip_model, blip_processor, correction_text, image, optimizer, epochs=1):\n",
    "    blip_model.train()\n",
    "    if not correction_text:\n",
    "        print(\"No correction provided. Skipping BLIP2 fine-tuning.\")\n",
    "        return blip_model\n",
    "\n",
    "    inputs = blip_processor(images=image, text=correction_text, return_tensors=\"pt\").to(device, dtype=torch.float32)\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    labels[labels == blip_processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = blip_model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"[LoRA Fine-tune] Epoch {epoch+1}/{epochs} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    blip_model.eval()\n",
    "    return blip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab32421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_image(image: Image.Image, correction_text: str, trigger_finetune: bool,target_language: str):\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "\n",
    "    yolo_results = yolo_model(image)\n",
    "    object_details = [yolo_model.names[int(cls)] for result in yolo_results for cls in result.boxes.cls.tolist() if result.boxes and result.boxes.cls.numel() > 0]\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        blip_inputs = blip_processor(images=image, text=[\"Describe this image.\"], return_tensors=\"pt\").to(device, dtype=torch.float32)\n",
    "        blip_output = blip_model.generate(**blip_inputs, max_new_tokens=60)\n",
    "        initial_caption = blip_processor.tokenizer.decode(blip_output[0], skip_special_tokens=True)\n",
    "\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "\n",
    "    if trigger_finetune and correction_text.strip():\n",
    "        print(\"Fine-tuning BLIP2 with user correction...\")\n",
    "        optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, blip_model.parameters()), lr=1e-4\n",
    "        )\n",
    "        fine_tune_blip2(blip_model, blip_processor, correction_text, image, optimizer)\n",
    "\n",
    "\n",
    "    object_summary = \", \".join(f\"{obj} ({count})\" for obj, count in Counter(object_details).items())\n",
    "    prompt = f\"Describe the scene in detail. Caption: '{initial_caption}'. The scene includes: {object_summary}. Write a descriptive paragraph (not a list).\"\n",
    "\n",
    "    # prompt = (\n",
    "    #     f\"A descriptive paragraph about this scene:\\n\"\n",
    "    #     f\"{initial_caption}\\n\"\n",
    "    #     f\"The scene includes: {object_summary}.\"\n",
    "    # )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        llama_inputs = llama_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        llama_ids = llama_model.generate(**llama_inputs, max_new_tokens=200, do_sample=True, temperature=0.8, top_k=50, top_p=0.95, repetition_penalty=1.2)\n",
    "        refined_caption = llama_tokenizer.decode(llama_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "    language_code_map = {\n",
    "        \"French\": \"fra_Latn\",\n",
    "        \"Spanish\": \"spa_Latn\",\n",
    "        \"German\": \"deu_Latn\",\n",
    "        \"Hindi\": \"hin_Deva\",\n",
    "        \"Arabic\": \"arb_Arab\",\n",
    "        \"Chinese (Simplified)\": \"zho_Hans\",\n",
    "        \"Malayalam\": \"mal_Mlym\"\n",
    "    }\n",
    "\n",
    "    lang_token_id = nllb_tokenizer.convert_tokens_to_ids(language_code_map.get(target_language, \"fra_Latn\"))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        nllb_inputs = nllb_tokenizer(refined_caption, return_tensors=\"pt\").to(device)\n",
    "        nllb_tokens = nllb_model.generate(**nllb_inputs, forced_bos_token_id=lang_token_id)\n",
    "        translated_caption = nllb_tokenizer.decode(nllb_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    tts = gTTS(translated_caption)\n",
    "    tts.save(\"caption_audio.mp3\")\n",
    "\n",
    "    return initial_caption, refined_caption, translated_caption, \"caption_audio.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72896f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interface = gr.Interface(\n",
    "    fn=describe_image,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Upload Image\"),\n",
    "        gr.Textbox(label=\"✏️ Your Caption Correction (Optional)\", placeholder=\"Enter improved caption if needed...\"),\n",
    "        gr.Checkbox(label=\"✅ Fine-Tune BLIP2 using this correction?\"),\n",
    "        gr.Dropdown(\n",
    "            choices=[\"French\", \"Spanish\", \"German\", \"Hindi\", \"Arabic\", \"Chinese (Simplified)\", \"Malayalam\"],\n",
    "            label=\"🌍 Select Translation Language\",\n",
    "            value=\"French\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"📌 Initial Caption (BLIP2)\"),\n",
    "        gr.Textbox(label=\"🪄 Refined Description (LLaMA)\"),\n",
    "        gr.Textbox(label=\"🌍 Translated Description\"),\n",
    "        gr.Audio(label=\"🔊 Description Audio\")\n",
    "    ],\n",
    "    title=\"🖼️ Image Detailed Description Generator\",\n",
    "    description=\"Upload an image and optionally provide a better caption to improve the model using fine-tuning.\"\n",
    ").queue()\n",
    "\n",
    "interface.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f7a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b4e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
